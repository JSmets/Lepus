{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone proposal - Deep Learning for Lepus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem to be solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are the main project idea and goals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By thinking about the capstone project, one emerged when discussing with prof. Yves Hausser from the nature management department at hepia (HES-SO Geneva) about a deep learning project of natural images. As follows, an introduction of this project is presented:\n",
    "\n",
    "The Lepus software [1] was designed to help scientists analyze wildlife images acquired using photographic traps. At present, species recognition and individual identification are carried out manually, which is very time-consuming. Note that, to get the data, local people have to access to these cameras and are present on some images. Thus, humans are also a category in itself and must be classified correctly.\n",
    "\n",
    "The objective of this project is to test Deep Learning technology to automate certain tasks such as\n",
    "\n",
    "1. detecting the presence or not of an animal/human in the image\n",
    "2. locating animals/humans using bounding boxes \n",
    "3. identifying certain species given a fixed taxon level\n",
    "4. ideally, identify each individual animal of a specific species with respect to physical characteristics (e.g. to help the Wild Life Conservation Society) e.g. [Computer Vision for Wildlife Conservation (CVWC)](https://arxiv.org/pdf/1906.05586.pdf)\n",
    "\n",
    "**Project Proposal** *Detection of the presence (#1): Classification of an animal, human and empty classes in the images (trinomial classification)*\n",
    "\n",
    "Please let shortly motivate this choice. By solving this problematic, the saved time for nature management scientists could be very high (days of work) because only a small amount of images contains animals or humans (this will be more deepely detailed below).\n",
    "Moreover, the labelled data do not include the bounding boxes which excludes the second project (here I consider only supervised learning to ensure a validation metric) and the fourth one is limited to the significant inspection variance of the manual identification and the very low amount of data available. \n",
    "Although the final objective is to identify aumatically animal species (third project), this is let to future works and this project aims solving the most time consuming part; the detection the presence or not of an animal/human in the image.\n",
    "\n",
    "As follows, you can see the project cloud for more details: \n",
    "\n",
    "- [1] https://lepus.cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What story you would like to tell with the data and what would you like to achieve at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims analysing and developping an accurate method for identifying the presence of animal species or humans. The main question marks are:\n",
    "- How many data is available for each class (human/animal/empty) and in which proportion.\n",
    "- Which type of information is available, how can they be usefull for the objective and discuss potential issues.\n",
    "\n",
    "This leads to a data exploration in order to answer these questions. Since this project has multiple problematics, several assumptions are needed. \n",
    "\n",
    "In the end, the objective is to determine which information and method result to the best accuracy for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) What is the main motivation behind your project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea following Lepus project is to reduce the high time consuming manual detection, localization and identification of animal species in photographic traps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the size and format of the data that you plan to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data information*\n",
    "The labels and other images information are stored in a **CSV** file. Lets load this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>session_dir</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>sun_angle</th>\n",
       "      <th>file_period</th>\n",
       "      <th>event_id</th>\n",
       "      <th>prev_file_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>taxon_id</th>\n",
       "      <th>taxon_tsn</th>\n",
       "      <th>taxon_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23653</th>\n",
       "      <td>20937</td>\n",
       "      <td>2014/M5/M5_35/11150113.JPG</td>\n",
       "      <td>M5 2014</td>\n",
       "      <td>2014-11-15 09:17:00</td>\n",
       "      <td>-48.82</td>\n",
       "      <td>day</td>\n",
       "      <td>3782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>153</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Papio cynocephalus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20308</th>\n",
       "      <td>17591</td>\n",
       "      <td>2014/M5/M5_22/11100055.JPG</td>\n",
       "      <td>M5 2014</td>\n",
       "      <td>2014-11-10 04:48:00</td>\n",
       "      <td>-114.30</td>\n",
       "      <td>night</td>\n",
       "      <td>3415</td>\n",
       "      <td>17590.0</td>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21629</th>\n",
       "      <td>18912</td>\n",
       "      <td>2014/M5/M5_22/11140337.JPG</td>\n",
       "      <td>M5 2014</td>\n",
       "      <td>2014-11-14 00:00:00</td>\n",
       "      <td>-184.32</td>\n",
       "      <td>night</td>\n",
       "      <td>3451</td>\n",
       "      <td>18911.0</td>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18228</th>\n",
       "      <td>15511</td>\n",
       "      <td>2014/M5/M5_11/11300412.JPG</td>\n",
       "      <td>M5 2014</td>\n",
       "      <td>2014-11-30 11:08:00</td>\n",
       "      <td>-22.73</td>\n",
       "      <td>day</td>\n",
       "      <td>2976</td>\n",
       "      <td>15510.0</td>\n",
       "      <td>8</td>\n",
       "      <td>133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>9922</td>\n",
       "      <td>2013/M3/M3_03/CDY_0091.JPG</td>\n",
       "      <td>M3 2013</td>\n",
       "      <td>2013-07-25 13:39:00</td>\n",
       "      <td>10.12</td>\n",
       "      <td>day</td>\n",
       "      <td>1758</td>\n",
       "      <td>9921.0</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id                   file_path session_dir        file_datetime  \\\n",
       "23653    20937  2014/M5/M5_35/11150113.JPG     M5 2014  2014-11-15 09:17:00   \n",
       "20308    17591  2014/M5/M5_22/11100055.JPG     M5 2014  2014-11-10 04:48:00   \n",
       "21629    18912  2014/M5/M5_22/11140337.JPG     M5 2014  2014-11-14 00:00:00   \n",
       "18228    15511  2014/M5/M5_11/11300412.JPG     M5 2014  2014-11-30 11:08:00   \n",
       "12648     9922  2013/M3/M3_03/CDY_0091.JPG     M3 2013  2013-07-25 13:39:00   \n",
       "\n",
       "       sun_angle file_period  event_id  prev_file_id  session_id  place_id  \\\n",
       "23653     -48.82         day      3782           NaN           8       153   \n",
       "20308    -114.30       night      3415       17590.0           8       143   \n",
       "21629    -184.32       night      3451       18911.0           8       143   \n",
       "18228     -22.73         day      2976       15510.0           8       133   \n",
       "12648      10.12         day      1758        9921.0          16        62   \n",
       "\n",
       "      taxon_id taxon_tsn          taxon_name  \n",
       "23653        6       NaN  Papio cynocephalus  \n",
       "20308        3       NaN                 NaN  \n",
       "21629      NaN       NaN                 NaN  \n",
       "18228      NaN       NaN                 NaN  \n",
       "12648      NaN       NaN                 NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_file = '../data/Lepus/csv_files/DeepLearningExport.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "data.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where columns means:\n",
    "* **flie_id** *[unique integer]*: the identifier (ID) of the file\n",
    "* **file_path** *[string]*: the path of the file image ({year}/{grid}/{camera}/{picture})\n",
    "* **session_dir** *[string]*: the directory name of the file (called session)\n",
    "* **file_datetime** *[timestamp]*: the date time of the file\n",
    "* **sun_angle** *[float]*: angle of the sun with 0° meaning sunset\n",
    "* **file_period** *[nominal/cyclic ordinal]*: the period of the file (day/night/twilight)\n",
    "* **event_id** *[integer]*: ID of the independent capture evenment (ICE)\n",
    "* **pred_file_id** *[float (integer+NaNs)]*: ID of the previous file deduced from event with the same ID\n",
    "* **session_id** *[integer]*: ID of the session\n",
    "* **place_id** *[integer]*: ID of the camera's location\n",
    "* **taxon_id** *[float (integer+NaNs)]*: Id of the taxon (i.e. species or lower in animal ranking)\n",
    "* **taxon_tsn** *[float (integer+NaNs)]*: ID of the world official tsn (taxonomic serial number)\n",
    "* **taxon_name** *[string]*: common name of the taxon present in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has shape: (32541, 13)\n"
     ]
    }
   ],
   "source": [
    "print('The data has shape:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Images*\n",
    "The data set is given as image files separated in several folders and subfolders. These folder are classified by year/grid/camera/picture where a grid correspond to a set of 36 cameras (6x6). \n",
    "\n",
    "The total dataset represents a covered area of 10'000 km 2  with hundred of cameras placed in different nature spots in Tanzania. The entire dataset consist of **32k** color and grayscale images. All these cameras are placed in different nature spots in Tanzania. This data has been obtained since 2013 and until 2016 (more recent data remains unlabelled). \n",
    "\n",
    "![Examples of images from the M1 2015 dataset.](images/imagesExamplesSeed3.png)\n",
    "\n",
    "![Examples of images from the M1 2015 dataset.](images/imagesExamplesSeed4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given species can be very small depending on the animal size and its distance to the camera or very large taking a large part of the image. Animals can be **occluded** by background objects (trees) of even be **partially viewed** (especially for large animals such as giraffes or elephants). In rare cases, it is possible to have more than one individual or more than one species in some images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exploratory data analysis*\n",
    "\n",
    "#### Data imbalancing\n",
    "Depending on the type of environment and the acquisition sensitivity of the photographic traps, the distribution of the classes empty/human/animal can be significantly imbalanced (see figure as follows).\n",
    "\n",
    "![Example of data imbalancing of the full dataset.](images/dataImbalancing2.png)\n",
    "\n",
    "The amount of species are in irregular proportion depending on the rarity of these species. Moreover, a majority of the given images doesn't contain any animals due to trees movement, dust tornado, butterflies, etc causing false positive captures. The proportion of empty pictures is **~40-100%** depending on the device environment.\n",
    "\n",
    "In the context of presence identification, the **data imbalancing** is an important issue. Indeed, if the data is used as is, the importance of very common label can be significantly overestimated. Indeed, the systematic identification of images into the label *empty* representing 47% of the data set results in an accuracy of 47%. The step of data balancing will be performed in order to induce no assuptions on the class distribution.\n",
    "\n",
    "In addition to this class imbalancing, there is animal species imbalancing inside the animal class due to the rarity of certain species (cf. following figure). This imbalancing should also be considered.\n",
    "\n",
    "![Example of data imbalancing of the full dataset.](images/species_percentage.png)\n",
    "\n",
    "The first 5 species represent in quantity almost half of the species present in the data. Lets look at these species.\n",
    "\n",
    "![Five most common species of the full dataset.](images/five_most_common_species.png)\n",
    "\n",
    "These species will have the most impact on the animal classification and rare species will have minor significance.\n",
    "\n",
    "#### Temporal information (ICE)\n",
    "Some of the images are time correlated due to animals running and get captured several times, i.e. **small timelaps images**. These set of images are already grouped by the Lepus software. These grouped images are called **independent capture event (ICE)** and numbered from 3 up to ~300 if an animal stays in front of the camera a long part of day/night.\n",
    "\n",
    "Since data is ordered by ICE (idependent capture events), there is a temporal information that can be used. In a special case, it is possible to see animals and humans moving using the difference between successive images and can especially show their presence or not (cf. following figure). Each event identifier contains from one to hundreds of pictures. Based on the file data time, each file_id is linked to the previous file identifier. If the value is a NaN, the picture is the first of the event. To work with images differences, we need at least 2 images per independent event.\n",
    "\n",
    "![Example of timelaps images and their difference from the M1 2015 dataset.](images/timelaps.png)\n",
    "\n",
    "Another interesting this to observe is the amount of images per events (cf. figure below).\n",
    "\n",
    "![Histogram of amount of images per events of the full dataset.](images/events_hist.png)\n",
    "\n",
    "Zooming on the histogram, the majority of the events contain 2 images. 2400 of them contain 1 image and are not usable for image differences. \n",
    "\n",
    "#### Additionnal information\n",
    "The information of animals lifestyle is a usefull assumption used by scientists to identify species in practice. Since the camera traps are well located and include **timesteps, the day, night and twilight** can be known (see below for further details). This can be used by adding this knowledge to infer the classification e.g. humans are (almost) always present during the day (see the following figure). To this end, it is possible to experiment and determine if this information can be interesting and how it could be in the context of human/animal/empty classification.\n",
    "\n",
    "![Day/night/twilight information of the full dataset.](images/categoryDayNightTwilight.png)\n",
    "\n",
    "Note that the day/night/twilight are deduced from the solar angle which is more precise. This more precise information can be used also.\n",
    "\n",
    "![Solar angle information of the full dataset.](images/categorySolarAngle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, cameras can have different fields of view (FOV) and resolutions including RGB and graylevel images. The latter comes from the difference between day and night acquisition devices. Note that this results to **highly non uniform representation of species** w.r.t. different situations (day/night, background situation, etc.). As an example some species can be only nocturne. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) How do you expect to get, manage and process the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Getting the data*\n",
    "The data is shared by physical transfer in order to conserve privacy. Images are separated into several folders as explained above, their paths and other information are specified into some CSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preparing the data*\n",
    "\n",
    "As a first step, the CSV files will be read into DataFrames and merged together (as shown above). Then this DataFrame will be cleaned and manipulated. After checking issues on formats and values of the data (nominal, ordinal and timestamp, ...), class labels will be grouped into human, animal and empty labels. This allows to compute the labels proportions and other statistics such as the proportion of day/night/twilight images w.r.t. species. \n",
    "\n",
    "Feature engineering will be performed to how use day/night/twilight as powerfull information for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data pre-processing*\n",
    "\n",
    "Images also needs processing. First of all, image will be normalized:\n",
    "- resize each images by downsampling the large input image into an adapted smaller shape. This will be performed empirically to balance between complexity and information loss.\n",
    "- normalize pixel intensities. In this case, in addition to 0-1 scaling, an adaptative contrast adjustment (CLAHE from cv2) will be computed to adjust contrast and enhance locally the visibility of hidden (dark) species. Note that this also enhances the noise in low contrast areas. \n",
    "\n",
    "#### *Data augmentation*\n",
    "\n",
    "The animal often represent a sub part of the image, this can induce backgroud learning instead of the animal. To reduce this by adding some variance to the data, some data modification is needed such as noise addition, horizontal flip, rotations or shear. Other potentially usefull methods but problematic will not be used because they could remove small species such as cut out or cropping.\n",
    "\n",
    "#### *Additional information processing*\n",
    "The additional data of the day/night/twilight is cyclic. Similarly to time data, it is possible to ensure locally constant difference by encoding them as day:1, twilight:0 and night:-1. This allows day and night being the highest difference possible. This idea will be analyzed in EDA over the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are the main challenges that you envision for completing the project and how do you plan to get around each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed project focuses on the classification of animal, human and empty in the pictures. To perform this and by looking at the data questions arise. \n",
    "\n",
    "Is there a difference in the final result if RGB or grayscale images are used ? Even more, what is the gain or the loss obtained after using the difference between 2 images following in a sequence ? Grayscale images differences or RBG images differences? These are the main project ideas and goals. Indeed, if 2 images follow one after the other with a relatively short amount of time separating them, the background would substract and the result image would focus on the differences between them. For example, the movement of an animal.\n",
    "\n",
    "To perform these analyses, our machine learning models will be trained on :\n",
    "\n",
    "* Grayscale images\n",
    "* Colour images\n",
    "* Grayscale images differences\n",
    "* Colour images differences\n",
    "* Day/night/twilight as additionnal information\n",
    "* Solar angle as additionnal information\n",
    "\n",
    "A second question would be which model to use ? Is there a model which is more efficient with images differences than row images ? What about grayscale and RBG ? For this purpose several models will be used.\n",
    "\n",
    "* k-nearest neighbours\n",
    "* Decision trees and random forest\n",
    "* Support vector machines\n",
    "* Fully connected neural networks\n",
    "* Convolutional neural network (pretrained for time optimization)\n",
    "\n",
    "As we saw during the fourth project, training over well defined features increase the overall accuracy of the machine learning models. For this purpose and to keep the scope of the project on the main goals, the pixels values will be used as input for each model.\n",
    "\n",
    "As a final challenge, can we combine some of the best trained models together and some additional information (i.e.  to further improve prediction results. In this last part, several methods will be experimented.\n",
    "\n",
    "* combine \n",
    "  * best image model\n",
    "  * image difference model\n",
    "  * additionnal information model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What the are steps that you plan to take to achieve the end goals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. Transfer Learning, models structures and trainings approaches*\n",
    "\n",
    "Instead of training on a randomly initialized model, it is common to perform transfer learning on a large challenging task pre-trained model (e.g. 1000-class [ImageNet](http://www.image-net.org/)). In addition of reducing the time of training, it also helps generalization (cf. [this paper](https://arxiv.org/pdf/1411.1792.pdf)). To experiment this, the following training will be performed:\n",
    "1. without transfer learning (same structure with randomly initialized weights)\n",
    "2. by fine-tuning (and freezing the other layers) with transfer learning on:\n",
    "    1. the last fully connected layers (this assumes the source and target domains are the same) and use the model as feature extractor.\n",
    "    2. on additionnal last layers (to be more task specific if both domains are different)\n",
    "\n",
    "For transfer learning, one very promising model has recently been published, on June 10th 2019, and shared, on [tensorflow](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), by Google Research called [EfficientNet-B0](https://arxiv.org/pdf/1905.11946.pdf). This model can be scaled depending on the computer learning capability and has already been trained on 1000-class [ImageNet](http://www.image-net.org/). This can be an ideal starting point for this project.\n",
    "\n",
    "Note that since it is also possible to experiment several other classifiers (e.g. decision tree, logistic regression, random forest, SVM, etc.), for the sake of simplicity the models will be trained only using dense layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2. Additional Information as Improvment using Machine Learning*\n",
    "\n",
    "\n",
    "Since the photographic traps are precisely located with timestamp, there is the information of solar elevation angle, thus the time of the day (day, night, twilight), which can be a powerfull information because certain species have specific lifestyle. The following experiments will be tested to compare the accuracy between the basline CNN model:\n",
    "1. without additional information\n",
    "2. with concatenating directly the first fully connected (fc) layer with the information of (day, night, twilight)\n",
    "3. with concatenating the solar elevation angle (computation allowing day/night/twilight changes at the approximate same time) using different machine learning methods:\n",
    "    * decision tree\n",
    "    * logistic regression\n",
    "    * random forest\n",
    "    * SVM\n",
    "    * dense (fc) layer\n",
    "\n",
    "The solar elevation angle should give more information since it is a (cyclic) continuous variable, in contrary to day,night,twilight which is a (cyclic) nominal feature.\n",
    "\n",
    "The following example shows the structure of the data concatenaton (e.g. here with fc layers).\n",
    "\n",
    "![Example of the additional information utilisation as improvment on a grid (M1 2015) with 518 species or humans.](images/CNN_additional_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Show us that you have a pipeline in place and that you understand the feasibility of your project goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. Data Loading, Cleaning and Manipulation:*\n",
    "- Load CSV file and get images pathnames in several folder and subfolders using recusrive search.\n",
    "\n",
    "- Clean data by removing unlabelled data and any without species in it.\n",
    "\n",
    "- Manage the remaining data for easy access and processing (e.g. flow from directory).\n",
    "\n",
    "#### *2. Images Preprocessing:*\n",
    "- Preform EDA to determine best methods for using the images and the additional information (day/night/twilight).\n",
    "\n",
    "- Select state of the art preprocessing methods adapted to our problematic (e.g.**\\*** histogram equalisation, denoising, resizing, data augmentation such as noise, horizontal flip, rotations or shear).\n",
    "\n",
    "- Implement them and make them easy to use such as with flow from directory.\n",
    "\n",
    "*\\*based on iWildCam 2019 challenge [Top7 report](https://github.com/Walleclipse/iWildCam_2019_FGVC6/blob/master/iwildcam_2019_report.pdf).*\n",
    "\n",
    "#### *3. Spliting the data:*\n",
    "- Separate the data into a stratified train/validation/test sets in order to conserve the labels (species) proportions and enviroment proportions, thus by keeping its global imbalance. This will be balanced or not depending on the experiments.\n",
    "\n",
    "#### *4. Transfer Learning:* \n",
    "- Select a state of the art pretrained model.\n",
    "\n",
    "- Adapt the structure to our problem by adjusting last layers to fit our desired output.\n",
    "\n",
    "#### *5. Baseline model:*\n",
    "- Train the adapted model on our dataset.\n",
    "\n",
    "- Validate the pre-trained model.\n",
    "\n",
    "- Set this trained model as the baseline for our future experiments.\n",
    "\n",
    "#### *6. Experiments for our final goal:*\n",
    "- A. Experiment data imbalacing to set the best preprocessing method for training.\n",
    "\n",
    "- B. Apply transfer learning, models structures and trainings approaches in order to improve the learning rate and/or the test accuracy.\n",
    "\n",
    "- C. Use additional information to try to further improve the final accuracy. \n",
    "\n",
    "#### *7. Obtain our final model by comining best results:*\n",
    "- Use the knowledge learned by all the experiments to define a final model and compute its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sample will be implemented in several python scripts (.py) for convenience and simplicity (e.g. separation of pre-processing, training and testing). An additional notebook (.ipynb) will be included with analysis details and visualization figures as a small report (similarly to this document).\n",
    "\n",
    "#### *Contribution of the proposed project*\n",
    "\n",
    "The proposed project of classifying animal species in images containing always at least one species. This work will differ on several points w.r.t. the state of the art [1,2].\n",
    "\n",
    "To obtain the top3 private score in iWildCam2019, [1] used a model ensemble of Inceptionv3, ResNet152, and InceptionResnetv2 and majority voting for classification. Moreover, they focused on methods for classifying and locating animals presence and absence in images (using DL [1] or image processing [2]) where the proposed project ignores this and assumes this is already performed. All these points will not be performed to focus on different points:\n",
    "\n",
    "* data imbalancing which was not experimented and analzed precisely.\n",
    "* using the entire image instead of a crop centered on the species.\n",
    "* concentrating the utilization on additional data information (e.g. day/night/twilight) thing they not used.\n",
    "* applying only one model instead of optimizing the accuracy exclusively using the input images.\n",
    "\n",
    "#### *References:*\n",
    "\n",
    "- [1] https://github.com/HayderYousif/iwildcam-2019-fgvc6, Github link.\n",
    "- [2] Yousif, Hayder, et al. \"Animal Scanner: Software for classifying humans, animals, and empty frames in camera trap images.\" Ecology and Evolution (2019).\n",
    "\n",
    "#### *Others*\n",
    "\n",
    "Note that the data can need to be confidential with a DNA (standard Non-Disclosure Agreement) due to some very rare species which are often hunted for money. If this is the case, it will be demanded to the EPFL soon. But anyways the precise location of the picture will not be transmitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
